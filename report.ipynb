{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c851a6d",
   "metadata": {},
   "source": [
    "## Assignment 2 – Practical Deep Learning Workshop\n",
    "### Final Report\n",
    "\n",
    "\n",
    "**Course:** Deep Learning  \n",
    "**name:** nehoray chalfon\n",
    "**ID:** 325833531"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53296976",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction and Dataset Overview\n",
    "\n",
    "### 1.1 Task Description\n",
    "\n",
    "This report presents our approach to the **Human Activity Recognition (HAR)** competition. The objective is to classify **18 distinct human activities** from smartphone **3-axis accelerometer data**. This is a **multi-class classification** problem where we predict the activity being performed based on raw sensor time series.\n",
    "\n",
    "### 1.2 Dataset Statistics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Training Samples** | 50,248 |\n",
    "| **Test Samples** | 74,744 |\n",
    "| **Number of Users (Train)** | 8 (user01 - user08) |\n",
    "| **Number of Classes** | 18 |\n",
    "| **Sensor Type** | 3-axis Accelerometer (x, y, z) |\n",
    "| **Sequence Length** | Variable (500 - 6,000+ timesteps) |\n",
    "| **Data Files** | Individual CSV files per sample |\n",
    "\n",
    "### 1.3 Activity Classes\n",
    "\n",
    "The 18 activity classes span multiple categories:\n",
    "\n",
    "| Category | Activities |\n",
    "|----------|------------|\n",
    "| **Walking** | walking_freely, walking_with_handbag, walking_holding_a_tray, walking_with_hands_in_pockets, walking_with_object_underarm |\n",
    "| **Stairs** | stairs_up, stairs_down |\n",
    "| **Hand Activities** | typing, writing, reading_book, using_phone, using_remote_control |\n",
    "| **Kitchen Tasks** | preparing_sandwich, washing_mug, washing_plate |\n",
    "| **Hygiene** | brushing_teeth, washing_face_and_hands |\n",
    "| **Stationary** | idle |\n",
    "\n",
    "### 1.4 Classification Objective\n",
    "\n",
    "This is a **multi-class classification** problem where we predict the **current activity** being performed (not a future event). The model receives a time series of accelerometer readings and must output one of 18 activity labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9137aecf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Exploratory Data Analysis Summary\n",
    "\n",
    "*Full EDA available in `eda.ipynb`*\n",
    "\n",
    "### 2.1 Key Findings\n",
    "\n",
    "#### (i) What is the type of this data? What does it represent?\n",
    "- **Data Type:** Time series of 3-axis accelerometer readings (x, y, z coordinates)\n",
    "- **Representation:** Each sample captures smartphone sensor data during a specific human activity\n",
    "- **Format:** Individual CSV files with variable-length sequences\n",
    "\n",
    "#### (ii) Is it homogeneous or does it vary in some way?\n",
    "- **Heterogeneous Data:**\n",
    "  - **Variable sequence lengths:** Ranges from ~500 to 6,000+ timesteps per sample\n",
    "  - **Two file types detected:**\n",
    "    - *Type 1:* Raw sensor readings with \"measurement type\" column (acceleration m/s²)\n",
    "    - *Type 2:* Derived position data (x, y, z in meters)\n",
    "  - **User variability:** Each user has distinct motion patterns (gait, hand movements)\n",
    "\n",
    "#### (iii) How was the data labeled?\n",
    "- Data was labeled by the **activity type** being performed\n",
    "- Labels were assigned per sample (one activity label per entire time series)\n",
    "- Labeling appears to have been done during controlled data collection sessions\n",
    "\n",
    "#### (iv) Should all labels be treated equally?\n",
    "- **Class Imbalance Exists:**\n",
    "  - Most frequent: \"idle\" (~9% of samples)\n",
    "  - Some activities have fewer samples than others\n",
    "  - Walking variants have similar motion patterns → higher confusion\n",
    "- **Recommendation:** Consider class weights or stratified sampling\n",
    "\n",
    "#### (v) How was the data split to train/test?\n",
    "- **Training data:** 8 users (user01 - user08)\n",
    "- **Test data:** Includes users **not present** in training set\n",
    "- **Critical Insight:** This is a **user-based split** where models must generalize to unseen users\n",
    "\n",
    "### 2.2 Class Distribution\n",
    "\n",
    "From our EDA, the activity distribution shows:\n",
    "\n",
    "| Activity | Samples | Percentage |\n",
    "|----------|---------|------------|\n",
    "| idle | ~4,600 | 9.2% |\n",
    "| walking_freely | ~3,800 | 7.6% |\n",
    "| typing | ~3,500 | 7.0% |\n",
    "| ... | ... | ... |\n",
    "| *Least frequent* | ~1,500 | 3.0% |\n",
    "\n",
    "### 2.3 Sample Signal Visualization\n",
    "\n",
    "Each sample consists of 3 channels (x, y, z acceleration) with the following characteristics:\n",
    "- **Walking activities:** Show periodic oscillations corresponding to step patterns\n",
    "- **Stationary activities:** Show low amplitude, noisy signals\n",
    "- **Transitions:** Some activities show distinct patterns at start/end\n",
    "\n",
    "*See `eda.ipynb` for detailed visualizations and plots.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a71542",
   "metadata": {},
   "source": [
    "### 2.4 Visualizations\n",
    "\n",
    "![Figure 1: Training Data Activity Distribution](figures/figure1_activity_distribution.png)\n",
    "\n",
    "**Key Observation:** Class imbalance ratio (max/min) is approximately 4:1. Walking activities and idle have the most samples.\n",
    "\n",
    "---\n",
    "\n",
    "![Figure 2: User Analysis](figures/figure2_user_analysis.png)\n",
    "\n",
    "**Key Observation:** All 8 training users have relatively balanced sample counts (5,500-7,400 samples each). The heatmap shows that some activities (stairs_down, stairs_up, typing) have zero samples for certain users.\n",
    "\n",
    "---\n",
    "\n",
    "![Figure 3: Train/Test Split Analysis](figures/figure3_train_test_split.png)\n",
    "\n",
    "**Critical Finding:** The train/test split is **user-based** with **zero overlap**. Training contains 8 users while the test set contains 21 different users. This means our model must generalize to completely unseen users.\n",
    "\n",
    "---\n",
    "\n",
    "![Figure 4: Sample Accelerometer Signals by Activity](figures/figure4_sample_signals.png)\n",
    "\n",
    "**Key Observations:**\n",
    "- **Idle:** Low amplitude, relatively stable signals\n",
    "- **Walking activities:** High amplitude oscillations with periodic patterns corresponding to steps\n",
    "- **Typing/Reading:** Low amplitude with occasional small movements\n",
    "- **Brushing teeth:** Distinctive repetitive high-frequency patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed896dd0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Self-Supervised Pretraining Tasks\n",
    "\n",
    "As seen in class, self-supervised learning can help models learn useful representations from unlabeled data before fine-tuning on the classification task. We propose two tasks suitable for accelerometer time series:\n",
    "\n",
    "### 3.1 Task 1: Masked Reconstruction\n",
    "\n",
    "**Objective:** Mask random portions of the time series and train the model to reconstruct them.\n",
    "\n",
    "**Implementation:**\n",
    "1. Take an input sequence of length T with 3 channels (x, y, z)\n",
    "2. Randomly mask 15-25% of consecutive timesteps (similar to BERT's MLM)\n",
    "3. Train an encoder-decoder to reconstruct the masked values\n",
    "4. Use MSE loss between predicted and original values\n",
    "\n",
    "**Why it helps:**\n",
    "- Forces the model to learn **temporal dependencies** and patterns\n",
    "- Model must understand activity-specific motion dynamics to predict missing parts\n",
    "- The encoder learns transferable features for classification\n",
    "\n",
    "**Architecture suggestion:** Transformer encoder with a linear decoder head\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Task 2: Contrastive Learning (SimCLR-style)\n",
    "\n",
    "**Objective:** Learn representations where augmented versions of the same sample are similar, while different samples are dissimilar.\n",
    "\n",
    "**Implementation:**\n",
    "1. For each sample, create two augmented views using:\n",
    "   - Time jittering (add Gaussian noise)\n",
    "   - Magnitude scaling (multiply by random factor 0.9-1.1)\n",
    "   - Time warping (speed up/slow down segments)\n",
    "2. Pass both views through an encoder to get embeddings\n",
    "3. Use NT-Xent (Normalized Temperature-scaled Cross Entropy) loss:\n",
    "   - Pull together embeddings from same sample (positive pairs)\n",
    "   - Push apart embeddings from different samples (negative pairs)\n",
    "\n",
    "**Why it helps:**\n",
    "- Learns **activity-invariant representations** robust to sensor variations\n",
    "- Captures inherent structure of activities regardless of user-specific patterns\n",
    "- Particularly useful for generalizing to unseen users\n",
    "\n",
    "**Architecture suggestion:** 1D-CNN or Transformer encoder with projection head\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Comparison\n",
    "\n",
    "| Task | Type | Key Benefit | Complexity |\n",
    "|------|------|-------------|------------|\n",
    "| Masked Reconstruction | Generative | Learns temporal dynamics | Medium |\n",
    "| Contrastive Learning | Discriminative | Learns invariant features | High |\n",
    "\n",
    "**Recommendation:** Start with contrastive learning as it directly encourages user-invariant features, which is crucial given our user-based train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882af93d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Validation Strategy\n",
    "\n",
    "### 4.1 Critical Insight from EDA\n",
    "\n",
    "Our exploratory analysis revealed a crucial finding:\n",
    "\n",
    "> **The test set contains users NOT present in the training set.**\n",
    "\n",
    "This means a model that memorizes user-specific patterns will fail when evaluated on the test set. We must design our validation strategy to simulate this scenario.\n",
    "\n",
    "### 4.2 Chosen Strategy: Leave-One-User-Out Cross-Validation (LOUO)\n",
    "\n",
    "**Approach:**\n",
    "- Use **LeaveOneGroupOut** with user ID as the group variable\n",
    "- Each fold uses **7 users for training**, **1 user for validation**\n",
    "- Total of **8 folds** (one per user)\n",
    "- Final score = average across all 8 folds\n",
    "\n",
    "**Why this works:**\n",
    "1. Simulates the test scenario where we encounter unseen users\n",
    "2. Prevents overfitting to user-specific motion patterns\n",
    "3. Provides robust performance estimates for real-world generalization\n",
    "\n",
    "### 4.3 Cross-Validation Splits\n",
    "\n",
    "| Fold | Training Users | Validation User | Train Size | Val Size |\n",
    "|------|----------------|-----------------|------------|----------|\n",
    "| 1 | user02-08 | user01 | ~44,000 | ~6,200 |\n",
    "| 2 | user01, user03-08 | user02 | ~44,000 | ~6,200 |\n",
    "| 3 | user01-02, user04-08 | user03 | ~44,000 | ~6,200 |\n",
    "| ... | ... | ... | ... | ... |\n",
    "| 8 | user01-07 | user08 | ~44,000 | ~6,200 |\n",
    "\n",
    "### 4.4 For Faster Iteration: 3-Fold GroupKFold\n",
    "\n",
    "Due to computational constraints, we also used a faster variant:\n",
    "- **GroupKFold with 3 splits** (user-based groups)\n",
    "- Maintains user separation but reduces training time by ~60%\n",
    "- Used for neural network hyperparameter tuning\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GroupKFold, LeaveOneGroupOut\n",
    "\n",
    "# Full LOUO (8 folds)\n",
    "cv_full = LeaveOneGroupOut()\n",
    "\n",
    "# Fast iteration (3 folds)\n",
    "cv_fast = GroupKFold(n_splits=3)\n",
    "```\n",
    "\n",
    "### 4.5 Validation Strategy Summary\n",
    "\n",
    "| Strategy | Folds | Time | Usage |\n",
    "|----------|-------|------|-------|\n",
    "| LeaveOneGroupOut | 8 | ~3x longer | Final evaluation |\n",
    "| GroupKFold (n=3) | 3 | ~1x | Development & tuning |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cd1afa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Naïve Baseline Results\n",
    "\n",
    "Before building complex models, we establish **naïve baselines** to set minimum performance thresholds.\n",
    "\n",
    "### 5.1 Baseline 1: Random Prediction (Class Prior)\n",
    "\n",
    "**Method:** Predict each class with probability proportional to its frequency in the training data.\n",
    "\n",
    "```python\n",
    "# Predict based on training class distribution\n",
    "y_pred = np.random.choice(classes, size=n_samples, p=class_probabilities)\n",
    "```\n",
    "\n",
    "**Result:**\n",
    "- **Accuracy:** 6.5% ± 0.5%\n",
    "- **F1-Score (Macro):** 5.2%\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 Baseline 2: Most Frequent Class\n",
    "\n",
    "**Method:** Always predict the most frequent class (\"idle\").\n",
    "\n",
    "```python\n",
    "# Always predict 'idle'\n",
    "y_pred = ['idle'] * n_samples\n",
    "```\n",
    "\n",
    "**Result:**\n",
    "- **Accuracy:** 9.2% ± 1.0%\n",
    "- **F1-Score (Macro):** 1.0%\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 Reference: Random Chance\n",
    "\n",
    "With 18 classes, random chance gives: $\\frac{1}{18} = 5.56\\%$\n",
    "\n",
    "---\n",
    "\n",
    "### 5.4 Baseline Comparison\n",
    "\n",
    "| Baseline | Accuracy | F1 (Macro) | Notes |\n",
    "|----------|----------|------------|-------|\n",
    "| Random Chance | 5.6% | - | Theoretical minimum |\n",
    "| Random (Class Prior) | 6.5% | 5.2% | Slightly above random |\n",
    "| **Most Frequent Class** | **9.2%** | 1.0% | **Threshold to beat** |\n",
    "\n",
    "---\n",
    "\n",
    "### 5.5 Key Takeaway\n",
    "\n",
    "> **Any useful model must achieve > 9.2% accuracy.**\n",
    "\n",
    "This Most Frequent baseline establishes the minimum threshold. If a model cannot beat this, it has learned nothing meaningful from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a311ced3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Classical ML with Feature Engineering\n",
    "\n",
    "### 6.1 Feature Engineering Approach\n",
    "\n",
    "Instead of feeding raw time series to a model, we extracted **70 handcrafted statistical features** from each sample:\n",
    "\n",
    "#### Time Domain Features (per axis: x, y, z, and magnitude)\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| Mean, Std | Basic statistics |\n",
    "| Min, Max, Range | Extremes |\n",
    "| Percentiles (25, 50, 75) | Distribution |\n",
    "| IQR | Interquartile range |\n",
    "| Skewness, Kurtosis | Shape of distribution |\n",
    "| Zero-Crossing Rate | Signal frequency indicator |\n",
    "| Mean Absolute Deviation | Variability measure |\n",
    "| RMS | Root mean square energy |\n",
    "| Energy | Sum of squared values |\n",
    "\n",
    "#### Frequency Domain Features\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| Spectral Energy | Total power in frequency domain |\n",
    "| Dominant Frequency | Most prominent frequency component |\n",
    "| Spectral Centroid | Center of mass of spectrum |\n",
    "\n",
    "#### Cross-Axis Features\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| Correlation (xy, xz, yz) | Relationships between axes |\n",
    "| Jerk (mean, std, max) | First derivative statistics |\n",
    "\n",
    "**Total: 70 features per sample**\n",
    "\n",
    "---\n",
    "\n",
    "### 6.2 Random Forest Classifier\n",
    "\n",
    "We trained a **Random Forest** with the following configuration:\n",
    "\n",
    "```python\n",
    "RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6.3 Results (Leave-One-User-Out CV)\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Mean Validation Accuracy** | **58.6% ± 4.8%** |\n",
    "| **Mean Validation F1 (Macro)** | **57.4% ± 8.2%** |\n",
    "| Mean Training Accuracy | 99.9% |\n",
    "\n",
    "**Improvement over baseline:** $(58.6 - 9.2) / 9.2 = $ **537% improvement!**\n",
    "\n",
    "---\n",
    "\n",
    "### 6.4 Per-Fold Results\n",
    "\n",
    "| Fold | Val User | Val Accuracy | Val F1 |\n",
    "|------|----------|--------------|--------|\n",
    "| 1 | user01 | 55.2% | 52.1% |\n",
    "| 2 | user02 | 61.3% | 58.7% |\n",
    "| 3 | user03 | 58.9% | 56.2% |\n",
    "| 4 | user04 | 54.7% | 51.8% |\n",
    "| 5 | user05 | 62.1% | 61.4% |\n",
    "| 6 | user06 | 59.8% | 58.3% |\n",
    "| 7 | user07 | 60.2% | 59.1% |\n",
    "| 8 | user08 | 56.4% | 62.3% |\n",
    "\n",
    "---\n",
    "\n",
    "### 6.5 Top 10 Most Important Features\n",
    "\n",
    "| Rank | Feature | Importance |\n",
    "|------|---------|------------|\n",
    "| 1 | mag_mean | 0.089 |\n",
    "| 2 | mag_std | 0.076 |\n",
    "| 3 | z_mean | 0.065 |\n",
    "| 4 | mag_energy | 0.058 |\n",
    "| 5 | x_std | 0.052 |\n",
    "| 6 | jerk_mean | 0.048 |\n",
    "| 7 | mag_rms | 0.045 |\n",
    "| 8 | y_range | 0.042 |\n",
    "| 9 | spectral_energy | 0.038 |\n",
    "| 10 | z_zcr | 0.035 |\n",
    "\n",
    "**Observation:** Magnitude-based features and signal energy are most discriminative for activity classification.\n",
    "\n",
    "---\n",
    "\n",
    "### 6.6 Key Takeaway\n",
    "\n",
    "> **Handcrafted features are extremely powerful for HAR.**\n",
    "\n",
    "The Random Forest achieved **58.6% accuracy** - significantly better than deep learning models with raw sequences. This demonstrates that domain knowledge in feature engineering can outperform end-to-end learning when data is limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9160505b",
   "metadata": {},
   "source": [
    "![Figure 5: Classical ML Results (Random Forest)](figures/figure5_random_forest.png)\n",
    "\n",
    "**Key Finding:** Random Forest achieved **58.6% accuracy** - a 537% improvement over the most frequent baseline! The most important features are magnitude-based (mag_mean, mag_std) and signal energy measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471981c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Neural Network Models (CNN & BiLSTM)\n",
    "\n",
    "We implemented two neural network architectures to learn directly from raw time series data.\n",
    "\n",
    "### 7.1 Model 1: 1D Convolutional Neural Network (CNN1D)\n",
    "\n",
    "**Architecture:**\n",
    "\n",
    "```\n",
    "Input: (batch, 3, seq_len)  # 3 channels: x, y, z\n",
    "    ↓\n",
    "Conv1D(3→64, k=7, s=2) + BatchNorm + ReLU + MaxPool\n",
    "    ↓\n",
    "Conv1D(64→128, k=5, s=1) + BatchNorm + ReLU + MaxPool\n",
    "    ↓\n",
    "Conv1D(128→256, k=3, s=1) + BatchNorm + ReLU + MaxPool\n",
    "    ↓\n",
    "Conv1D(256→256, k=3, s=1) + BatchNorm + ReLU + AdaptiveAvgPool\n",
    "    ↓\n",
    "Flatten → Linear(256→128) → ReLU → Dropout(0.5)\n",
    "    ↓\n",
    "Linear(128→18) → Output\n",
    "```\n",
    "\n",
    "**Parameters:** ~375,000\n",
    "\n",
    "**Key Design Choices:**\n",
    "- Increasing filter sizes to capture multi-scale temporal patterns\n",
    "- BatchNorm for stable training\n",
    "- Global Average Pooling for variable-length input handling\n",
    "- Dropout for regularization\n",
    "\n",
    "---\n",
    "\n",
    "### 7.2 Model 2: Bidirectional LSTM (BiLSTM)\n",
    "\n",
    "**Architecture:**\n",
    "\n",
    "```\n",
    "Input: (batch, seq_len, 3)  # Transposed from CNN format\n",
    "    ↓\n",
    "BiLSTM(input=3, hidden=128, layers=2, bidirectional=True)\n",
    "    ↓\n",
    "Attention Layer: Linear(256→64) → Tanh → Linear(64→1) → Softmax\n",
    "    ↓\n",
    "Weighted Sum across time (context vector: 256-dim)\n",
    "    ↓\n",
    "Linear(256→128) → ReLU → Dropout(0.3)\n",
    "    ↓\n",
    "Linear(128→18) → Output\n",
    "```\n",
    "\n",
    "**Parameters:** ~583,000\n",
    "\n",
    "**Key Design Choices:**\n",
    "- Bidirectional processing captures both past and future context\n",
    "- Attention mechanism learns which timesteps are most important\n",
    "- Deeper network (2 layers) for complex temporal patterns\n",
    "\n",
    "---\n",
    "\n",
    "### 7.3 Training Configuration\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Batch Size | 64 |\n",
    "| Learning Rate | 0.001 |\n",
    "| Optimizer | Adam |\n",
    "| Scheduler | ReduceLROnPlateau (patience=2, factor=0.5) |\n",
    "| Max Epochs | 15 |\n",
    "| Early Stopping | patience=4 |\n",
    "| Sequence Length | 1,500 (padded/truncated) |\n",
    "| Training Samples | 10,000 (stratified subset) |\n",
    "| Cross-Validation | 3-Fold GroupKFold |\n",
    "\n",
    "**Hardware:** NVIDIA GTX 1050 Ti (4GB), CUDA 12.1\n",
    "\n",
    "---\n",
    "\n",
    "### 7.4 Results\n",
    "\n",
    "| Model | Val Accuracy | Val F1 (Macro) | Training Time |\n",
    "|-------|-------------|----------------|---------------|\n",
    "| **1D-CNN** | **48.5% ± 1.5%** | **47.8% ± 2.1%** | ~21 min/fold |\n",
    "| BiLSTM | 35.3% ± 4.3% | 33.2% ± 5.1% | ~21 min/fold |\n",
    "\n",
    "---\n",
    "\n",
    "### 7.5 Training Curves\n",
    "\n",
    "Both models showed typical learning patterns:\n",
    "- **CNN:** Steady convergence, small train-val gap (good generalization)\n",
    "- **LSTM:** More volatile training, larger train-val gap (overfitting tendency)\n",
    "\n",
    "---\n",
    "\n",
    "### 7.6 Analysis: Why CNN Outperformed LSTM\n",
    "\n",
    "1. **Local Patterns Matter More:** Walking and hand activities have distinctive local temporal signatures (e.g., step patterns, repetitive motions) that convolutions capture effectively.\n",
    "\n",
    "2. **LSTM Limitations:**\n",
    "   - Sequences are very long (1,500 timesteps) → vanishing gradients\n",
    "   - LSTMs need more data to learn long-range dependencies\n",
    "   - Attention helps but adds complexity\n",
    "\n",
    "3. **Parameter Efficiency:** CNN has fewer parameters but still outperformed the larger LSTM.\n",
    "\n",
    "---\n",
    "\n",
    "### 7.7 Comparison with Random Forest\n",
    "\n",
    "| Model | Val Accuracy | Approach |\n",
    "|-------|-------------|----------|\n",
    "| Random Forest | 58.6% | Features + Classical ML |\n",
    "| 1D-CNN | 48.5% | Raw sequences + Deep Learning |\n",
    "| BiLSTM | 35.3% | Raw sequences + Deep Learning |\n",
    "\n",
    "**Observation:** Random Forest with handcrafted features still outperforms end-to-end deep learning by **+10% accuracy**. This suggests:\n",
    "- Our dataset size (10K samples) may be insufficient for deep learning\n",
    "- Feature engineering captures domain knowledge that models must learn from scratch\n",
    "- Hybrid approaches (CNN features + RF) could be promising"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66bacb4",
   "metadata": {},
   "source": [
    "![Figure 6: Neural Network Results](figures/figure6_neural_networks.png)\n",
    "\n",
    "**Key Findings:**\n",
    "- CNN (48.5%) outperformed LSTM (35.3%)\n",
    "- Both neural networks underperformed Random Forest (58.6%)\n",
    "- Training curves show CNN has better generalization (smaller train-val gap)\n",
    "- LSTM shows signs of overfitting with larger train-val divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447f917e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Pretrained Model Fine-tuning (Chronos)\n",
    "\n",
    "### 8.1 Transfer Learning Approach\n",
    "\n",
    "We leveraged **Amazon Chronos T5-Small**, a foundation model for time series pretrained on diverse forecasting tasks.\n",
    "\n",
    "**Model:** `amazon/chronos-t5-small`\n",
    "- **Architecture:** T5 (Text-to-Text Transfer Transformer) adapted for time series\n",
    "- **Pretraining:** Trained on massive time series forecasting datasets\n",
    "- **Original Task:** Time series forecasting (predict future values)\n",
    "\n",
    "---\n",
    "\n",
    "### 8.2 Adaptation Strategy\n",
    "\n",
    "**Challenge:** Chronos is designed for forecasting, not classification.\n",
    "\n",
    "**Solution:** Freeze the pretrained encoder and add a trainable classification head.\n",
    "\n",
    "```\n",
    "Input: 3-axis accelerometer (x, y, z)\n",
    "    ↓\n",
    "Compute magnitude: √(x² + y² + z²)  # Convert to univariate\n",
    "    ↓\n",
    "Downsample to 384 timesteps (Chronos expected input size)\n",
    "    ↓\n",
    "Chronos T5 Encoder (FROZEN - 46M parameters)\n",
    "    ↓\n",
    "Global Average Pooling → 512-dim embedding\n",
    "    ↓\n",
    "Classification Head (TRAINABLE):\n",
    "    Linear(512→512) → ReLU → Dropout(0.4)\n",
    "    Linear(512→256) → ReLU → Dropout(0.3)\n",
    "    Linear(256→18) → Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8.3 Parameter Efficiency\n",
    "\n",
    "| Component | Parameters | Trainable? |\n",
    "|-----------|------------|------------|\n",
    "| Chronos T5 Encoder | 46,000,000 |  Frozen |\n",
    "| Classification Head | ~400,000 |  Trainable |\n",
    "| **Total** | **46,400,000** | **0.8% trained** |\n",
    "\n",
    "This is a highly **parameter-efficient** approach: we only train 0.8% of the model!\n",
    "\n",
    "---\n",
    "\n",
    "### 8.4 Training Configuration\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Sample Size | 3,000 |\n",
    "| Sequence Length | 384 (downsampled) |\n",
    "| Batch Size | 64 |\n",
    "| Learning Rate | 0.003 (higher for classifier head) |\n",
    "| Epochs | 12 |\n",
    "| Validation Split | 80/20 (user-based) |\n",
    "\n",
    "---\n",
    "\n",
    "### 8.5 Results\n",
    "\n",
    "The fine-tuning showed promising learning dynamics, with the classification head gradually learning to map Chronos embeddings to activity labels.\n",
    "\n",
    "**Key Observations:**\n",
    "1. **Learning Curve:** The model showed steady improvement over epochs\n",
    "2. **Domain Gap:** Chronos was pretrained on forecasting, not classification\n",
    "3. **Univariate Limitation:** Converting 3-axis data to magnitude loses directional information\n",
    "\n",
    "---\n",
    "\n",
    "### 8.6 Comparison with Other Models\n",
    "\n",
    "| Model | Training Strategy | Val Accuracy |\n",
    "|-------|-------------------|--------------|\n",
    "| Random Forest | Feature Engineering | 58.6% |\n",
    "| 1D-CNN | Train from Scratch | 48.5% |\n",
    "| BiLSTM | Train from Scratch | 35.3% |\n",
    "| Chronos (Fine-tuned) | Transfer Learning | ~45-50%* |\n",
    "\n",
    "*Chronos results varied based on training configuration.\n",
    "\n",
    "---\n",
    "\n",
    "### 8.7 Analysis and Lessons Learned\n",
    "\n",
    "**Why Transfer Learning Had Mixed Results:**\n",
    "\n",
    "1. **Domain Mismatch:** Chronos learned forecasting patterns (predict future values), not classification patterns (categorize sequences).\n",
    "\n",
    "2. **Modality Difference:** Time series forecasting operates on continuous values, while classification requires categorical outputs.\n",
    "\n",
    "3. **Sequence Conversion:** Converting 3-axis data to univariate magnitude loses important directional information.\n",
    "\n",
    "**What Could Help:**\n",
    "- Fine-tune a HAR-specific pretrained model (e.g., trained on other HAR datasets)\n",
    "- Use a model pretrained with contrastive learning on sensor data\n",
    "- Keep full 3-axis input with multi-channel adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e0752",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Error Analysis and Improvement Suggestions\n",
    "\n",
    "### 9.1 Confusion Matrix Analysis\n",
    "\n",
    "Analyzing the confusion matrix of our best model (Random Forest), we identified systematic error patterns:\n",
    "\n",
    "#### Most Confused Activity Pairs\n",
    "\n",
    "| True Activity | Predicted As | Confusion Rate |\n",
    "|---------------|--------------|----------------|\n",
    "| walking_with_handbag | walking_freely | 18% |\n",
    "| walking_with_hands_in_pockets | walking_freely | 15% |\n",
    "| walking_holding_a_tray | walking_freely | 14% |\n",
    "| washing_mug | washing_plate | 22% |\n",
    "| using_phone | typing | 12% |\n",
    "| reading_book | typing | 11% |\n",
    "\n",
    "**Pattern:** Activities within the same category (walking variants, sitting activities, kitchen tasks) are frequently confused.\n",
    "\n",
    "---\n",
    "\n",
    "### 9.2 Per-Class Performance\n",
    "\n",
    "| Class | Precision | Recall | F1-Score | Support |\n",
    "|-------|-----------|--------|----------|---------|\n",
    "| walking_freely | 0.72 | 0.68 | 0.70 | ~3,800 |\n",
    "| idle | 0.85 | 0.91 | 0.88 | ~4,600 |\n",
    "| typing | 0.61 | 0.58 | 0.59 | ~3,500 |\n",
    "| stairs_down | 0.78 | 0.71 | 0.74 | ~2,200 |\n",
    "| washing_plate | 0.45 | 0.42 | 0.43 | ~1,800 |\n",
    "| ... | ... | ... | ... | ... |\n",
    "\n",
    "**Best performing:** idle, stairs_down (distinct motion patterns)\n",
    "**Worst performing:** kitchen activities, walking variants (similar patterns)\n",
    "\n",
    "---\n",
    "\n",
    "### 9.3 Three Improvement Suggestions\n",
    "\n",
    "Based on our error analysis, we propose three strategies to improve model performance:\n",
    "\n",
    "#### Suggestion 1: Data Augmentation\n",
    "\n",
    "**Problem:** Limited training data leads to overfitting and poor generalization.\n",
    "\n",
    "**Solution:** Apply random transformations during training:\n",
    "- **Jittering:** Add Gaussian noise (σ = 0.05)\n",
    "- **Scaling:** Multiply by random factor (0.9 - 1.1)\n",
    "- **Time Warping:** Speed up/slow down random segments\n",
    "\n",
    "**Expected Benefit:** Forces model to learn robust features, reduces overfitting, improves generalization to unseen users.\n",
    "\n",
    "**Difficulty:** Easy to implement, low computational overhead.\n",
    "\n",
    "---\n",
    "\n",
    "#### Suggestion 2: Deeper Architecture (ResNet-1D)\n",
    "\n",
    "**Problem:** Simple 4-layer CNN may underfit complex activity patterns.\n",
    "\n",
    "**Solution:** Implement ResNet-style architecture with:\n",
    "- Residual connections (skip connections)\n",
    "- 8 residual blocks\n",
    "- Deeper feature extraction\n",
    "\n",
    "**Expected Benefit:** Better gradient flow enables training deeper networks, potentially capturing more complex temporal hierarchies.\n",
    "\n",
    "**Difficulty:** Medium - requires architectural changes.\n",
    "\n",
    "---\n",
    "\n",
    "#### Suggestion 3: Ensemble Methods\n",
    "\n",
    "**Problem:** Different models capture different aspects of the data.\n",
    "\n",
    "**Solution:** Combine predictions from multiple models:\n",
    "- Random Forest (statistical features)\n",
    "- CNN (local temporal patterns)\n",
    "- LSTM (sequential dependencies)\n",
    "\n",
    "**Approach:** Weighted average of predicted probabilities or voting.\n",
    "\n",
    "**Expected Benefit:** Reduced variance, more robust predictions, leverages complementary strengths.\n",
    "\n",
    "**Difficulty:** Easy to implement, but increases inference time.\n",
    "\n",
    "---\n",
    "\n",
    "### 9.4 Prioritization\n",
    "\n",
    "| Rank | Suggestion | Impact | Effort | Priority |\n",
    "|------|------------|--------|--------|----------|\n",
    "| 1 | Data Augmentation | High | Low | High |\n",
    "| 2 | ResNet-1D | Medium | Medium | Medium |\n",
    "| 3 | Ensemble | Medium | Low | Low |\n",
    "\n",
    "**We implemented suggestions 1 and 2 in Part 2g (see next section).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acec030",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Implemented Improvements and Final Results\n",
    "\n",
    "We implemented the top 2 prioritized improvements and evaluated their impact.\n",
    "\n",
    "### 10.1 Improvement 1: Data Augmentation\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "```python\n",
    "class AugmentedDataset(Dataset):\n",
    "    def __init__(self, X, y, augment=False):\n",
    "        self.X = torch.FloatTensor(X).permute(0, 2, 1)\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.X[idx], self.y[idx]\n",
    "        \n",
    "        if self.augment:\n",
    "            # Jittering: Add Gaussian noise\n",
    "            if torch.rand(1) < 0.5:\n",
    "                noise = torch.randn_like(x) * 0.05\n",
    "                x = x + noise\n",
    "            \n",
    "            # Scaling: Random magnitude change\n",
    "            if torch.rand(1) < 0.5:\n",
    "                scale = 1.0 + (torch.rand(1) - 0.5) * 0.2  # 0.9 to 1.1\n",
    "                x = x * scale\n",
    "        \n",
    "        return x, y\n",
    "```\n",
    "\n",
    "**Result:** CNN with augmentation achieved **49.6% accuracy** (+1.1% improvement over baseline CNN)\n",
    "\n",
    "---\n",
    "\n",
    "### 10.2 Improvement 2: ResNet-1D Architecture\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "```python\n",
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=18):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.conv1 = nn.Conv1d(in_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual blocks (8 blocks total)\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)    # 2 blocks\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)   # 2 blocks\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)   # 2 blocks\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)   # 2 blocks\n",
    "        \n",
    "        # Classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "```\n",
    "\n",
    "**Parameters:** ~4.2M (vs ~375K for baseline CNN)\n",
    "\n",
    "**Result:** ResNet-1D achieved **48.4% accuracy** (comparable to baseline CNN)\n",
    "\n",
    "---\n",
    "\n",
    "### 10.3 Final Results Comparison\n",
    "\n",
    "| Model | Val Accuracy | Val F1 | vs Baseline CNN |\n",
    "|-------|-------------|--------|-----------------|\n",
    "| **Random Forest** | **58.6%** | **57.4%** | +10.1% |\n",
    "| **CNN + Augmentation** | **49.6%** | 47.7% | **+1.1%** |\n",
    "| ResNet-1D | 48.4% | 47.3% | -0.1% |\n",
    "| 1D-CNN (Baseline) | 48.5% | 47.8% | - |\n",
    "| BiLSTM | 35.3% | 33.2% | -13.2% |\n",
    "| Most Frequent | 9.2% | 1.0% | -39.3% |\n",
    "\n",
    "---\n",
    "\n",
    "### 10.4 Analysis of Improvement Results\n",
    "\n",
    "#### Data Augmentation Success\n",
    "\n",
    "The CNN + Augmentation model showed consistent improvement:\n",
    "- **+1.1% absolute accuracy improvement** over baseline CNN\n",
    "- Particularly effective for fold 1 (51.4% vs ~48%)\n",
    "- **Zero additional inference cost** - augmentation only applied during training\n",
    "\n",
    "**Why it worked:**\n",
    "- Jittering simulates sensor noise variations between users\n",
    "- Scaling simulates different motion intensities\n",
    "- Forces model to learn robust, generalizable features\n",
    "\n",
    "---\n",
    "\n",
    "#### ResNet-1D Performance\n",
    "\n",
    "The ResNet-1D achieved comparable results to the baseline CNN:\n",
    "- 48.4% accuracy (marginally below 48.5% baseline)\n",
    "- 11x more parameters but no improvement\n",
    "\n",
    "**Why it didn't improve:**\n",
    "1. **Insufficient data:** Deeper models need more training data, not all data was used due to time and compute constraints.\n",
    "2. **Limited training epochs:** 15 epochs may not be enough for 4.2M parameters\n",
    "3. **Overfitting risk:** More parameters with same data can lead to overfitting\n",
    "\n",
    "**What could help:**\n",
    "- Train for 50+ epochs with learning rate scheduling\n",
    "- Use more aggressive regularization (higher dropout, weight decay)\n",
    "- Increase training data with more aggressive augmentation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10094b25",
   "metadata": {},
   "source": [
    "![Figure 7: Final Results and Improvement Analysis](figures/figure7_final_results.png)\n",
    "\n",
    "**Final Results Summary:**\n",
    "\n",
    "| Model | Accuracy | F1-Score |\n",
    "|-------|----------|----------|\n",
    "| Random Forest | 58.6% | 57.4% |\n",
    "| CNN + Augmentation | 49.6% | 47.7% |\n",
    "| 1D-CNN (Baseline) | 48.5% | 47.8% |\n",
    "| ResNet-1D | 48.4% | 47.3% |\n",
    "| BiLSTM | 35.3% | 33.2% |\n",
    "| Most Frequent Baseline | 9.2% | 1.0% |\n",
    "\n",
    "**Key Takeaway:** Classical ML with handcrafted features (Random Forest) outperformed all deep learning approaches by +10% accuracy, demonstrating the value of domain expertise when training data is limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2419e5b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Conclusion\n",
    "\n",
    "### 11.1 Summary of Findings\n",
    "\n",
    "#### Best Performing Models\n",
    "\n",
    "| Rank | Model | Accuracy | Key Insight |\n",
    "|------|-------|----------|-------------|\n",
    "| 1 | Random Forest + Features | 58.6% | Handcrafted features excel with limited data |\n",
    "| 2 | CNN + Data Augmentation | 49.6% | Augmentation improves neural network generalization |\n",
    "| 3 | 1D-CNN (Baseline) | 48.5% | Solid baseline for end-to-end learning |\n",
    "\n",
    "#### Critical Success Factors\n",
    "\n",
    "1. **User-Based Validation:** Using Leave-One-User-Out CV was essential for realistic performance estimation. Models that don't account for user separation will overestimate their performance.\n",
    "\n",
    "2. **Feature Engineering:** Domain-specific features (magnitude, spectral energy, jerk) captured activity characteristics more effectively than raw sequences with limited data.\n",
    "\n",
    "3. **Data Augmentation:** Simple jittering and scaling provided consistent improvements with zero inference overhead.\n",
    "\n",
    "---\n",
    "\n",
    "### 11.2 What Worked vs. What Didn't\n",
    "\n",
    "| Worked Well | Didn't Meet Expectations |\n",
    "|---------------|---------------------------|\n",
    "| Random Forest with 70 features | BiLSTM (underperformed CNN) |\n",
    "| Data augmentation | ResNet-1D (no improvement over simple CNN) |\n",
    "| User-based cross-validation | Transfer learning from forecasting model |\n",
    "| 1D-CNN architecture | Very deep networks with limited data |\n",
    "\n",
    "---\n",
    "\n",
    "### 11.3 Recommendations for This Competition\n",
    "\n",
    "Based on the experiments, the recommendation is:\n",
    "\n",
    "1. **Primary Submission:** Random Forest with handcrafted features (58.6% accuracy)\n",
    "2. **Alternative:** CNN + Augmentation for faster inference if needed\n",
    "\n",
    "---\n",
    "\n",
    "### 11.4 Code\n",
    "\n",
    "**GitHub Repository:** [https://github.com/NehorayChalfon0166/deeplearning_ws](https://github.com/NehorayChalfon0166/deeplearning_ws)\n",
    "\n",
    "\n",
    "| Resource | Location |\n",
    "|----------|----------|\n",
    "| EDA Notebook | `eda.ipynb` |\n",
    "| Training Notebook | `model_training.ipynb` |\n",
    "| Report | `report.ipynb` (this file) |\n",
    "| Model Checkpoints | `checkpoints/` |\n",
    "| Experiment Tracking | W&B project: `har-deep-learning` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
